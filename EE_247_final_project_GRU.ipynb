{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "EE 247 final project_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiGI0306sEJX",
        "colab_type": "code",
        "outputId": "49e240f6-6dd8-44ed-fdb3-aca86bd6d194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKSF1YIpsmIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwjCfkAvljYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# for creating validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, ELU\n",
        "from torch.optim import Adam, SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6VgwMJXchke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzjOal5dljYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = np.load(\"X_test.npy\")\n",
        "y_test = np.load(\"y_test.npy\")\n",
        "person_train_valid = np.load(\"person_train_valid.npy\")\n",
        "X_train_valid = np.load(\"X_train_valid.npy\")\n",
        "y_train_valid = np.load(\"y_train_valid.npy\")\n",
        "person_test = np.load(\"person_test.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pNlBDcfljYP",
        "colab_type": "code",
        "outputId": "edc2cfae-3a28-425d-9de2-08692496bce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
        "print ('Person test shape: {}'.format(person_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training/Valid data shape: (2115, 22, 1000)\n",
            "Test data shape: (443, 22, 1000)\n",
            "Training/Valid target shape: (2115,)\n",
            "Test target shape: (443,)\n",
            "Person train/valid shape: (2115, 1)\n",
            "Person test shape: (443, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1OTW12fljYT",
        "colab_type": "code",
        "outputId": "791faeec-8dc2-43df-f2c1-ffb3d9a934e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_valid = X_train_valid[:, np.newaxis, :, :]\n",
        "X_test = X_test[:, np.newaxis, :, :]\n",
        "# create labels \n",
        "y_train_valid = y_train_valid - 769\n",
        "y_test = y_test -769\n",
        "\n",
        "# # # reshape to suit the input of GRU\n",
        "# X_train_valid = np.swapaxes(X_train_valid, 1, 2)\n",
        "\n",
        "# split training and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_valid, y_train_valid, test_size = 0.1)\n",
        "\n",
        "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(((1903, 1, 22, 1000), (1903,)), ((212, 1, 22, 1000), (212,)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgNpsbBkljYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transform to torch tensor\n",
        "X_train = torch.from_numpy(X_train)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "\n",
        "X_val = torch.from_numpy(X_val)\n",
        "y_val = torch.from_numpy(y_val)\n",
        "\n",
        "X_test = torch.from_numpy(X_test)\n",
        "y_test = torch.from_numpy(y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9xecE_dKDAz",
        "colab_type": "code",
        "outputId": "fb1d65d1-4f5c-4dc9-fc76-6597a0262441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "class GRUNet(nn.Module):\n",
        "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, \n",
        "                          dropout = drop_prob, batch_first=True)\n",
        "    \n",
        "        self.linear_layers = Sequential(\n",
        "            Linear(hidden_dim*n_layers, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            ReLU(inplace=True),\n",
        "            Linear(64, output_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "        # print(out.shape)\n",
        "        # print(h.shape)\n",
        "        out = self.linear_layers(h.transpose(0, 1).reshape(h.shape[1], -1))\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n",
        "\n",
        "# defining the model\n",
        "model = GRUNet(input_dim = 22, hidden_dim = 64, output_dim = 4, n_layers = 1)\n",
        "model = model.float()\n",
        "\n",
        "# defining the loss function\n",
        "criterion = CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRUNet(\n",
            "  (gru): GRU(22, 64, batch_first=True, dropout=0.2)\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Linear(in_features=64, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB39OL-lPkQb",
        "colab_type": "code",
        "outputId": "dd148294-7549-4325-8c29-6b14a22b956a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "## LSTM ####\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "\n",
        "        self.linear_layers = Sequential(\n",
        "            Linear(hidden_dim*n_layers, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            ReLU(inplace=True),\n",
        "            Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = h[1].transpose(0, 1).reshape(h[1].shape[1], -1)\n",
        "        out = self.linear_layers(out)\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden\n",
        "\n",
        "# defining the model\n",
        "model = LSTMNet(input_dim = 22, hidden_dim = 64, output_dim = 4, n_layers = 4)\n",
        "model = model.float()\n",
        "\n",
        "# defining the loss function\n",
        "criterion = CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "print(model)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMNet(\n",
            "  (lstm): LSTM(22, 64, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
            "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Linear(in_features=64, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDeFW7TqntJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net_lstm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_lstm, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1,10),  stride=(1,1))\n",
        "        self.conv1_bn = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(21,1), stride=(1,1))\n",
        "        self.conv2_bn = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,10), stride=(1,1))\n",
        "        self.conv3_bn = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1,10), stride=(1,1))\n",
        "        self.conv4_bn = nn.BatchNorm2d(128)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1,4), stride=(1,4), padding=(0,0))\n",
        "        self.relu = nn.ELU() \n",
        "        self.lstm1 = nn.LSTM(128*24, 64, 1, batch_first=True)#, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(64, 64, 1, batch_first=True)#, bidirectional=True)\n",
        "        self.lstm3 = nn.LSTM(128, 64, 1, batch_first=True)#, bidirectional=True)\n",
        "        self.lstm4 = nn.LSTM(192, 64, 1, batch_first=True)#, bidirectional=True)\n",
        "        self.fc = nn.Linear(in_features=64,  out_features=4)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.softmax = nn.Softmax()\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, X):\n",
        "        # Nx1x22x1000\n",
        "        x = self.conv1(X)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv1_bn(x) \n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2_bn(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3_bn(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv4_bn(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.permute(0,3,1,2) # important! -- without <50% 128x2x12->12x128x2\n",
        "        x = x.reshape(-1,1,12*128*2)\n",
        "        x, _ = self.lstm1(x)\n",
        "        res1 = x\n",
        "        x, _ = self.lstm2(x)\n",
        "        res2 = x\n",
        "        x = torch.cat((x, res1), dim=2)\n",
        "        x, _ = self.lstm3(x)\n",
        "        x = torch.cat((x, res1, res2), dim=2)\n",
        "        x,_ = self.lstm4(x)\n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpx8hcPgttcD",
        "colab_type": "code",
        "outputId": "2000beeb-68e7-46b0-85fe-3e2144e2391d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# defining the model\n",
        "model = Net_lstm()\n",
        "model = model.float()\n",
        "\n",
        "# defining the loss function\n",
        "criterion = CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net_lstm(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(1, 10), stride=(1, 1))\n",
            "  (conv1_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(21, 1), stride=(1, 1))\n",
            "  (conv2_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(1, 10), stride=(1, 1))\n",
            "  (conv3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(64, 128, kernel_size=(1, 10), stride=(1, 1))\n",
            "  (conv4_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=(0, 0), dilation=1, ceil_mode=False)\n",
            "  (relu): ELU(alpha=1.0)\n",
            "  (lstm1): LSTM(3072, 64, batch_first=True)\n",
            "  (lstm2): LSTM(64, 64, batch_first=True)\n",
            "  (lstm3): LSTM(128, 64, batch_first=True)\n",
            "  (lstm4): LSTM(192, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5a6626f8-c728-4349-c563-ef54109ee1a7",
        "id": "X4Uab9LJoNCp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        }
      },
      "source": [
        "# process the data in batches for training\n",
        "batch_size = 50\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "n_epochs = 50\n",
        "n_batches = len(y_train)//batch_size\n",
        "count = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print_every = n_batches // 10\n",
        "    total_train_loss = 0\n",
        "    for i,data in enumerate(train_loader, 0):\n",
        "        count += 1\n",
        "\n",
        "        # extract data in this batch\n",
        "        inputs, labels = data\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        \n",
        "        #Set the parameter gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #Forward pass, backward pass, optimize\n",
        "        out= model(inputs.float())\n",
        "        loss_train = criterion(out, labels.long())\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #train loss\n",
        "        total_train_loss += loss_train\n",
        "\n",
        "        #Print every 100 iteration of an epoch\n",
        "        if count % 50 == 0:\n",
        "            print(\"Epoch {}, Step: {}/{}, \\t train_loss: {:.2f} \".format(\n",
        "                    epoch+1, count, len(train_loader), total_train_loss))\n",
        "            \n",
        "    if (epoch+1) % 5 == 0:    \n",
        "        ## At the end of the epoch, do a pass on the validation set\n",
        "        out = model(X_val.cuda().float())\n",
        "        loss_val = criterion(out, y_val.to(device).long())\n",
        "        print(\"Epoch {}, \\t validation loss = {:.2f}\".format(epoch+1, loss_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 2, Step: 50/38, \t train_loss: 0.71 \n",
            "Epoch 3, Step: 100/38, \t train_loss: 1.29 \n",
            "Epoch 4, Step: 150/38, \t train_loss: 1.34 \n",
            "Epoch 5, \t validation loss = 2.38\n",
            "Epoch 6, Step: 200/38, \t train_loss: 0.29 \n",
            "Epoch 7, Step: 250/38, \t train_loss: 1.35 \n",
            "Epoch 8, Step: 300/38, \t train_loss: 2.55 \n",
            "Epoch 10, Step: 350/38, \t train_loss: 1.08 \n",
            "Epoch 10, \t validation loss = 2.49\n",
            "Epoch 11, Step: 400/38, \t train_loss: 1.33 \n",
            "Epoch 12, Step: 450/38, \t train_loss: 0.90 \n",
            "Epoch 14, Step: 500/38, \t train_loss: 0.11 \n",
            "Epoch 15, Step: 550/38, \t train_loss: 0.13 \n",
            "Epoch 15, \t validation loss = 2.52\n",
            "Epoch 16, Step: 600/38, \t train_loss: 0.20 \n",
            "Epoch 18, Step: 650/38, \t train_loss: 0.01 \n",
            "Epoch 19, Step: 700/38, \t train_loss: 0.02 \n",
            "Epoch 20, Step: 750/38, \t train_loss: 0.03 \n",
            "Epoch 20, \t validation loss = 2.70\n",
            "Epoch 22, Step: 800/38, \t train_loss: 0.00 \n",
            "Epoch 23, Step: 850/38, \t train_loss: 0.01 \n",
            "Epoch 24, Step: 900/38, \t train_loss: 0.02 \n",
            "Epoch 25, Step: 950/38, \t train_loss: 0.02 \n",
            "Epoch 25, \t validation loss = 2.78\n",
            "Epoch 27, Step: 1000/38, \t train_loss: 0.00 \n",
            "Epoch 28, Step: 1050/38, \t train_loss: 0.01 \n",
            "Epoch 29, Step: 1100/38, \t train_loss: 0.01 \n",
            "Epoch 30, \t validation loss = 2.86\n",
            "Epoch 31, Step: 1150/38, \t train_loss: 0.00 \n",
            "Epoch 32, Step: 1200/38, \t train_loss: 0.01 \n",
            "Epoch 33, Step: 1250/38, \t train_loss: 0.01 \n",
            "Epoch 35, Step: 1300/38, \t train_loss: 0.00 \n",
            "Epoch 35, \t validation loss = 2.89\n",
            "Epoch 36, Step: 1350/38, \t train_loss: 0.01 \n",
            "Epoch 37, Step: 1400/38, \t train_loss: 0.01 \n",
            "Epoch 39, Step: 1450/38, \t train_loss: 0.00 \n",
            "Epoch 40, Step: 1500/38, \t train_loss: 0.00 \n",
            "Epoch 40, \t validation loss = 2.93\n",
            "Epoch 41, Step: 1550/38, \t train_loss: 0.01 \n",
            "Epoch 43, Step: 1600/38, \t train_loss: 0.00 \n",
            "Epoch 44, Step: 1650/38, \t train_loss: 0.00 \n",
            "Epoch 45, Step: 1700/38, \t train_loss: 0.00 \n",
            "Epoch 45, \t validation loss = 2.98\n",
            "Epoch 47, Step: 1750/38, \t train_loss: 0.00 \n",
            "Epoch 48, Step: 1800/38, \t train_loss: 0.00 \n",
            "Epoch 49, Step: 1850/38, \t train_loss: 0.00 \n",
            "Epoch 50, Step: 1900/38, \t train_loss: 0.01 \n",
            "Epoch 50, \t validation loss = 3.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XyQ8aoMqYK7",
        "colab_type": "code",
        "outputId": "b6da18c2-d793-4132-a824-2e5ec5c86cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## validation accuracy\n",
        "with torch.no_grad():\n",
        "    output = model(X_val.cuda().float())\n",
        "softmax = torch.exp(output).cpu()\n",
        "prob = list(softmax.numpy())\n",
        "predictions = np.argmax(prob, axis=1)\n",
        "\n",
        "# accuracy on validation set\n",
        "accuracy_score(y_val.cpu(), predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5235849056603774"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmoGmQ_YvFix",
        "colab_type": "code",
        "outputId": "31a8dff2-d38e-48eb-b30e-c5c44966bda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## test accuracy\n",
        "with torch.no_grad():\n",
        "    output = model(X_test.to(device).cuda().float())\n",
        "softmax = torch.exp(output).cpu()\n",
        "prob = list(softmax.numpy())\n",
        "predictions = np.argmax(prob, axis=1)\n",
        "\n",
        "# accuracy on validation set\n",
        "accuracy_score(y_test.cpu(), predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5304740406320542"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kvo8E1apZXC",
        "colab_type": "code",
        "outputId": "0f2727bc-6c35-4101-e68c-afc8bac48a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## training procedure for LSTM and GRU (ONLY)\n",
        "# process the data in batches for training\n",
        "batch_size = 100\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-2)\n",
        "n_epochs = 10\n",
        "n_batches = len(y_train)//batch_size\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    running_loss = 0\n",
        "    print_every = n_batches // 10\n",
        "    total_train_loss = 0\n",
        "    h = model.init_hidden(batch_size)\n",
        "    for i,data in enumerate(train_loader, 0):\n",
        "        # extract data in this batch\n",
        "        inputs, labels = data\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        \n",
        "        #Set the parameter gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #Forward pass, backward pass, optimize\n",
        "        h = tuple([e.data for e in h])\n",
        "        out, h = model(inputs.float(), h)\n",
        "        loss_train = criterion(out, labels.long())\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #Print statistics\n",
        "        running_loss += loss_train\n",
        "        total_train_loss += loss_train\n",
        "        \n",
        "        #Print every 10th batch of an epoch\n",
        "        if (i + 1) % (print_every + 1) == 0:\n",
        "            print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} \".format(\n",
        "                    epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every))\n",
        "            #Reset running loss and time\n",
        "            running_loss = 0.0\n",
        "        \n",
        "    # #At the end of the epoch, do a pass on the validation set\n",
        "    h = model.init_hidden(len(y_val))\n",
        "    out, h = model(X_val.cuda().float(), h)\n",
        "    loss_val = criterion(out, y_val.to(device).long())\n",
        "\n",
        "    print(\"Validation loss = {:.2f}\".format(loss_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, 10% \t train_loss: 2.96 \n",
            "Epoch 1, 21% \t train_loss: 2.93 \n",
            "Epoch 1, 31% \t train_loss: 2.92 \n",
            "Epoch 1, 42% \t train_loss: 2.86 \n",
            "Epoch 1, 52% \t train_loss: 2.83 \n",
            "Epoch 1, 63% \t train_loss: 2.83 \n",
            "Epoch 1, 73% \t train_loss: 2.83 \n",
            "Epoch 1, 84% \t train_loss: 2.86 \n",
            "Epoch 1, 94% \t train_loss: 2.78 \n",
            "Validation loss = 1.38\n",
            "Epoch 2, 10% \t train_loss: 2.77 \n",
            "Epoch 2, 21% \t train_loss: 2.81 \n",
            "Epoch 2, 31% \t train_loss: 2.81 \n",
            "Epoch 2, 42% \t train_loss: 2.78 \n",
            "Epoch 2, 52% \t train_loss: 2.73 \n",
            "Epoch 2, 63% \t train_loss: 2.77 \n",
            "Epoch 2, 73% \t train_loss: 2.84 \n",
            "Epoch 2, 84% \t train_loss: 2.84 \n",
            "Epoch 2, 94% \t train_loss: 2.78 \n",
            "Validation loss = 1.39\n",
            "Epoch 3, 10% \t train_loss: 2.76 \n",
            "Epoch 3, 21% \t train_loss: 2.78 \n",
            "Epoch 3, 31% \t train_loss: 2.80 \n",
            "Epoch 3, 42% \t train_loss: 2.73 \n",
            "Epoch 3, 52% \t train_loss: 2.75 \n",
            "Epoch 3, 63% \t train_loss: 2.76 \n",
            "Epoch 3, 73% \t train_loss: 2.74 \n",
            "Epoch 3, 84% \t train_loss: 2.76 \n",
            "Epoch 3, 94% \t train_loss: 2.73 \n",
            "Validation loss = 1.41\n",
            "Epoch 4, 10% \t train_loss: 2.75 \n",
            "Epoch 4, 21% \t train_loss: 2.78 \n",
            "Epoch 4, 31% \t train_loss: 2.72 \n",
            "Epoch 4, 42% \t train_loss: 2.67 \n",
            "Epoch 4, 52% \t train_loss: 2.69 \n",
            "Epoch 4, 63% \t train_loss: 2.70 \n",
            "Epoch 4, 73% \t train_loss: 2.70 \n",
            "Epoch 4, 84% \t train_loss: 2.76 \n",
            "Epoch 4, 94% \t train_loss: 2.78 \n",
            "Validation loss = 1.43\n",
            "Epoch 5, 10% \t train_loss: 2.66 \n",
            "Epoch 5, 21% \t train_loss: 2.66 \n",
            "Epoch 5, 31% \t train_loss: 2.71 \n",
            "Epoch 5, 42% \t train_loss: 2.72 \n",
            "Epoch 5, 52% \t train_loss: 2.70 \n",
            "Epoch 5, 63% \t train_loss: 2.74 \n",
            "Epoch 5, 73% \t train_loss: 2.70 \n",
            "Epoch 5, 84% \t train_loss: 2.71 \n",
            "Epoch 5, 94% \t train_loss: 2.73 \n",
            "Validation loss = 1.45\n",
            "Epoch 6, 10% \t train_loss: 2.64 \n",
            "Epoch 6, 21% \t train_loss: 2.67 \n",
            "Epoch 6, 31% \t train_loss: 2.60 \n",
            "Epoch 6, 42% \t train_loss: 2.65 \n",
            "Epoch 6, 52% \t train_loss: 2.63 \n",
            "Epoch 6, 63% \t train_loss: 2.63 \n",
            "Epoch 6, 73% \t train_loss: 2.61 \n",
            "Epoch 6, 84% \t train_loss: 2.71 \n",
            "Epoch 6, 94% \t train_loss: 2.71 \n",
            "Validation loss = 1.47\n",
            "Epoch 7, 10% \t train_loss: 2.51 \n",
            "Epoch 7, 21% \t train_loss: 2.51 \n",
            "Epoch 7, 31% \t train_loss: 2.55 \n",
            "Epoch 7, 42% \t train_loss: 2.64 \n",
            "Epoch 7, 52% \t train_loss: 2.64 \n",
            "Epoch 7, 63% \t train_loss: 2.59 \n",
            "Epoch 7, 73% \t train_loss: 2.50 \n",
            "Epoch 7, 84% \t train_loss: 2.61 \n",
            "Epoch 7, 94% \t train_loss: 2.60 \n",
            "Validation loss = 1.50\n",
            "Epoch 8, 10% \t train_loss: 2.47 \n",
            "Epoch 8, 21% \t train_loss: 2.40 \n",
            "Epoch 8, 31% \t train_loss: 2.50 \n",
            "Epoch 8, 42% \t train_loss: 2.47 \n",
            "Epoch 8, 52% \t train_loss: 2.57 \n",
            "Epoch 8, 63% \t train_loss: 2.59 \n",
            "Epoch 8, 73% \t train_loss: 2.58 \n",
            "Epoch 8, 84% \t train_loss: 2.56 \n",
            "Epoch 8, 94% \t train_loss: 2.53 \n",
            "Validation loss = 1.52\n",
            "Epoch 9, 10% \t train_loss: 2.49 \n",
            "Epoch 9, 21% \t train_loss: 2.33 \n",
            "Epoch 9, 31% \t train_loss: 2.38 \n",
            "Epoch 9, 42% \t train_loss: 2.52 \n",
            "Epoch 9, 52% \t train_loss: 2.47 \n",
            "Epoch 9, 63% \t train_loss: 2.58 \n",
            "Epoch 9, 73% \t train_loss: 2.50 \n",
            "Epoch 9, 84% \t train_loss: 2.46 \n",
            "Epoch 9, 94% \t train_loss: 2.40 \n",
            "Validation loss = 1.62\n",
            "Epoch 10, 10% \t train_loss: 2.30 \n",
            "Epoch 10, 21% \t train_loss: 2.22 \n",
            "Epoch 10, 31% \t train_loss: 2.21 \n",
            "Epoch 10, 42% \t train_loss: 2.31 \n",
            "Epoch 10, 52% \t train_loss: 2.47 \n",
            "Epoch 10, 63% \t train_loss: 2.32 \n",
            "Epoch 10, 73% \t train_loss: 2.24 \n",
            "Epoch 10, 84% \t train_loss: 2.38 \n",
            "Epoch 10, 94% \t train_loss: 2.36 \n",
            "Validation loss = 1.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa2ZOmi2CWdY",
        "colab_type": "code",
        "outputId": "2604f952-3d53-488c-8d81-22f368e07b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "h[1].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 212, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J1RosTDg0en",
        "colab_type": "code",
        "outputId": "77feb843-da3c-4c94-d12a-7b95f613cea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "## validation accuracy\n",
        "h = model.init_hidden(len(y_val))\n",
        "with torch.no_grad():\n",
        "    output, h = model(X_val.cuda().float(), h)\n",
        "softmax = torch.exp(output).cpu()\n",
        "prob = list(softmax.numpy())\n",
        "predictions = np.argmax(prob, axis=1)\n",
        "\n",
        "# accuracy on validation set\n",
        "accuracy_score(y_val.cpu(), predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ef9e8f1eadcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Net_lstm' object has no attribute 'init_hidden'"
          ]
        }
      ]
    }
  ]
}